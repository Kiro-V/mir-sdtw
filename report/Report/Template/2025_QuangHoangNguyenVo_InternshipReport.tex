
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Setup title, etc in config.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{config}                     
\documentclass[a4paper,11pt,\myPageLayout]{book}
\usepackage[utf8]{inputenc}
\usepackage{captionSmall}

\input{config_packages_macros}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Start of document
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\fancypagestyle{plain}{\pagestyle{mine}} % remove this if you don't want 
                                         % headings on the first page of a chapter
\frontmatter
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title Page
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{titlepage}
\cleardoublepage{}

\pagenumbering{roman}
\pagestyle{mine}
\newpage

\section*{Abstract}
The evolution of Deep Neural Networks (DNNs) has shifted the paradigm of music information retrieval (MIR) from heuristic and mathematical models to data-driven approaches, which rely on large amounts of labelled training data.
However, it introduces challenges when training with weakly aligned datasets. In this project, we investigate the characteristics of differential dynamic time warping (dDTW) through the soft-DTW (sDTW) algorithm when training with weakly aligned data.
The main objective is to integrate soft-DTW as a loss function in the training process of a template-based chord recognition model.
The dataset will have its chord label timestamps distorted or removed to simulate weakly or unaligned data.
The dDTW loss function will then be used to train the model with the distorted dataset. The results will be compared with those obtained using the original dataset and the Connectionist Temporal Classification (CTC) loss function.
Additional tasks may include experimenting and evaluating the performance of sDTW with different stablizing strategies.
\mainmatter{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Table of Contents
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\setcounter{tocdepth}{1}
\setcounter{page}{1}
{\parskip=0mm \tableofcontents}
% \thispagestyle{empty}

\pagestyle{mine}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Chapters
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\cleardoublepage{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
\label{chapter:Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Amidst the rapid advancement of deep neural networks (DNNs), the field of music information retrieval (MIR) has witnessed a significant shift from traditional heuristic and mathematical models to data-driven approaches that heavily rely on large amounts of labelled training data, such as pitch estimation \cite{Kim2018_CREPEmodel_ISMIR}, audio embeddings \cite{Cramer2019_OpenL3_ISMIR}, automatic music transcription \cite{Benetos2019_AutomaticMusicTranscription}.

However, the reliance on large and accurate datasets poses many challenges, considering the time-consuming and labor-intensive nature of manual annotation, as well as the potential for human error and subjectivity. Thus, it is generally difficult to obtain strongly aligned annotations, where each frame of the audio signal is associated with a corresponding label. Instead, weakly aligned or unaligned annotations are more common, where only the presence or absence of certain labels is known, without precise temporal alignment. This ease up the data acquisition process, but requires a more sophisticated loss function to train the model. One proposed solution is using connectionist temporal classification (CTC) loss \cite{Graves2006_ConnectionistTemporalClassification_ICML}.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Soft Dynamic Time Warping Algorithm}
\label{chapter:sdtw}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In this chapter, we present the mathematical formulation of the soft-DTW algorithm.
\section{Definition and Notation}
Let $\mathbf{x} = (x_0, \ldots, x_{N-1}) \in \mathbb{R}^N$  and $\mathbf{y} = (y_0, \ldots, y_{M-1}) \in \mathbb{R}^M$ be two time series of representing the sequence of predictions and strong targets, respectively. We then denote the soft target sequence as $\mathbf{y'} = (y'_0, \ldots, y'_{M'-1}) \in \mathbb{R}^{M'}$, where $M' < M$. The objective of soft-DTW is to calculate the alignment cost matrix between $\mathbf{x}$ and $\mathbf{y'}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Forward Pass}
\label{section:forward_pass}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In the forward pass, we compute the accumulated cost matrix $\mathbf{D}(M,N) \in \mathbb{R}^{M \times N}$, where each element $D(i,j)$ represents the minimum cost of aligning the first $i$ elements of $\mathbf{y'}$ with the first $j$ elements of $\mathbf{x}$.
The accumulated cost is computed using the local cost matrix $\mathbf{C}$, where $\mathbf{C}(i,j) = c(x_i, y'_j)$, which measures the dissimilarity between the elements $x_i$ and $y'_j$. A common choice for the local cost function is the squared Euclidean distance:
\begin{equation}
    \mathbf{C}(i,j) = \| x_i - y'_j \|^2
\end{equation}
Instead of using the hard minimum operator as in traditional DTW, soft-DTW employs a differentiable approximation, defined as:
\begin{equation}
    \text{min}^\gamma = -\gamma \log \sum_{s \in S} \left( e^{-s/\gamma} \right)
\end{equation}
where $\gamma > 0$ is a smoothing parameter that controls the softness of the minimum operation, and $S$ is the set of values over which the soft minimum is computed.
Combining with the step constraint of DTW, we only consider three possible predecessor cells for each cell $(i,j)$, which are $(i-1,j)$, $(i,j-1)$, and $(i-1,j-1)$.
Thus the accumulated cost matrix $\mathbf{D}$ is computed recursively as follows:
\begin{equation}
    \mathbf{D}(i,j) = \mathbf{C}(i,j) + \text{min}^\gamma \left( \mathbf{D}(i-1,j), \mathbf{D}(i,j-1), \mathbf{D}(i-1,j-1) \right)
\end{equation}

The recursive algorithm is summarized in Algorithm \ref{algorithm:forward_pass}.

\begin{algorithm}[t]
\caption{Forward Pass of Soft-DTW}
\label{algorithm:forward_pass}
\begin{algorithmic}
\Require Time series $\mathbf{x} \in \mathbb{R}^N$, $\mathbf{y'} \in \mathbb{R}^{M'}$, smoothing parameter $\gamma > 0$
\Ensure Accumulated cost matrix $\mathbf{D} \in \mathbb{R}^{M' \times N}$
\State Initialize $\mathbf{D}(0,0) = 0$
\For{$i = 1$ to $M'$}
    \State $\mathbf{D}(i,0) = \infty$
\EndFor
\For{$j = 1$ to $N$}
    \State $\mathbf{D}(0,j) = \infty$
\EndFor
\For{$i = 1$ to $M'$}
    \For{$j = 1$ to $N$}
        \State Compute local cost: $\mathbf{C}(i,j) = \| x_j - y'_i \|^2$
        \State Update accumulated cost:
        \[
        \mathbf{D}(i,j) = \mathbf{C}(i,j) + \text{min}^\gamma \left( \mathbf{D}(i-1,j), \mathbf{D}(i,j-1), \mathbf{D}(i-1,j-1) \right)
        \]
    \EndFor
\EndFor

\Return $\mathbf{D}$
\end{algorithmic}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Backward Pass}
\label{section:backward_pass}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In order to train the model, we need to compute the gradients of the soft-DTW loss with respect to the model parameters. The backward pass computes the gradient $H \in \mathbb{R}^{M' \times N}$ of the soft-DTW cost with respect to the cost matrix $\mathbf{C} \in \mathbb{R}^{M' \times N}$.
An efficient way to compute the gradient is to use a dynamic programming approach similar to the forward pass. The gradient is computed recursively as follows:
\begin{equation}
    \mathbf{H}(i,j) = \frac{\partial \mathbf{D}(M',N)}{\partial \mathbf{D}(i,j)} \frac {\partial \mathbf{D}(i,j)}{\partial \mathbf{C}(i,j)}
\end{equation}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Experimental Setup}
\label{chapter:experimental_setup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In this section, we present the objectives of our experiment, the dataset used for training alongside with the network architecture and the training process.

\section{Dataset}
\label{section:dataset}
For the experiment, we use the Beatles dataset retrieved from Isophonics \cite{HarteSAG05_ChordRecognition_ISMIR}, consisting of four audio recordings with respective annotations. 
Since the original annotations have more than 24 chord types, which would make the network too complex and beyond the scope of this project.
We therefore consider the simplified version of such annotations, which reduced the number of chord types to only 24 (12 chromas with their respective major or minor variant)\cite{Mueller15_FundamentalsMusicProcessig_SPRINGER}.
We split the dataset into training, validation, and test sets. For test set, a short segment of Let It Be is used,
while the rest of the dataset is split into 3:1 ratio for training and validation.

We choose a sequence length of 150 samples for training and validating the model, creating 43 and 12 segments for training and validation, respectively.
In case of soft alignment, we remove the adjacent repetitions in the sequence, effectively reducing its length by around 85\% on average (see Figure \ref{figure:soft_alignment}).
However, this method introduce a problem where batching target sequences of different lengths is not possible. To address this issue, after reduction, we pad the sequences repeating each frame uniformly until they reach a desired length, or "soft length".
After some experiments, we found that a soft length of 16 covers all of possible reduced sequences while keeping the reduction ratio high.


\section{Model Architecture}
\label{section:model_architecture}
Given the aim of this experiment is to evaluate the performance of the proposed SDTW loss function, the network architecture plays a minor role and are kept simple.
Therefore, we used a simple chord recognition network (dChord) that based on the template-based chord recognition algorithm.
This network consists of a single layer that acts as the chord template to predict a 24-dimensional activation vector, corresponding to 24 chord types.
The network has a total of 25 trainable parameters.
Table \ref{table:arch} illustrate the components of the architecture with their respective input and output dimensions.

During training, we use Adam optimizer with a learning rate of 0.01.

\begin{table}[t]
    \begin{center}
    \begin{tabular}{|c|c|c|c|}
        \hline
        Layer & Input Dimension & Output Dimension & Parameters \\
        \hline
        Log-compression & (12, 150) & (12, 150) & 0 \\
        Normalization & (12, 150) & (12, 150) & 0 \\
        dChord & (12, 150) & (24, 150) & 25 \\
        softmax & (24, 150) & (24, 150) & 0 \\
        \hline
    \end{tabular}
        \caption{Architecture of the chord recognition network. T is the number of time frames.}
        \label{table:arch}
    \end{center}
\end{table}



\section{Results and Discussion}
\label{section:resultsdiscussion}

\section{Baseline: Strongly Aligned Data with Binary Cross-entropy Loss}
\label{section:baseline}

\section{Weakly Aligned Data with Soft-DTW Loss}
\label{section:weakly_aligned}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusions}
\label{chapter:conclusions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % Appendix
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \cleardoublepage{}
% \appendix
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \chapter{Source Code}
% \label{chapter:source_code}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% In this chapter, the headers of selected \MATLAB{} functions created during the writing of this thesis are reproduced. The headers contain information about the name of the described function and its input/output behavior.

% \section*{Feature Extraction}
% The \texttt{file\_to\_feature} function is used as a wrapper for several low-level functions that perform feature extraction or loading of precomputed features.

% Sample usage:\\
% \scriptsize
% \verb|[f_pitch, f_peaks] = file_to_feature('features', 'pathetique.wav');|

% \begin{verbatim}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % Name: file_to_feature
% % Version: 1.0
% % Date: 11.05.2010
% % Programmer: John Q. Public
% %
% % Description:
% %   Load or compute features for audio and MIDI files
% %
% % Input:
% % - dirname: Directory where the file or features are located
% % - filename: Name of the file for which to load/compute features
% % - parameter
% %            .win_len: Window length used for STMSP feature generation
% %            .win_res: Window resolution
% %
% % Output:
% % - f_pitch: Pitch features (STMSP)
% % - f_peaks: Energy peaks for onset computation
% % - f_onsets: Precise onsets (only generated in case of MIDI input data)
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \end{verbatim}
% \normalsize

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Bibliography
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\cleardoublepage{}
{
\small
\addcontentsline{toc}{chapter}{Bibliography}
\bibliographystyle{ieeetr}
\bibliography{bibliography}
}
\cleardoublepage{}


\end{document}
