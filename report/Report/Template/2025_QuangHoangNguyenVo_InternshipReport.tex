
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Setup title, etc in config.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{config}                     
\documentclass[a4paper,11pt,\myPageLayout]{book}
\usepackage[utf8]{inputenc}
\usepackage{captionSmall}

\input{config_packages_macros}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Start of document
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\fancypagestyle{plain}{\pagestyle{mine}} % remove this if you don't want 
                                         % headings on the first page of a chapter
\frontmatter
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title Page
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{titlepage}
\cleardoublepage{}

\pagenumbering{roman}
\pagestyle{mine}
\newpage

\section*{Abstract}
The evolution of Deep Neural Networks (DNNs) has shifted the paradigm of music information retrieval (MIR) from heuristic and mathematical models to data-driven approaches, which rely on large amounts of labelled training data.
However, it introduces challenges when training with weakly aligned datasets. In this project, we investigate the characteristics of differential dynamic time warping (dDTW) through the soft-DTW (sDTW) algorithm when training with weakly aligned data.
The main objective is to integrate soft-DTW as a loss function in the training process of a template-based chord recognition model.
The dataset will have its chord label timestamps distorted or removed to simulate weakly or unaligned data.
The dDTW loss function will then be used to train the model with the distorted dataset. The results will be compared with those obtained using the original dataset and the Connectionist Temporal Classification (CTC) loss function.
Additional tasks may include experimenting and evaluating the performance of sDTW with different stablizing strategies.
\mainmatter{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Table of Contents
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\setcounter{tocdepth}{1}
\setcounter{page}{1}
{\parskip=0mm \tableofcontents}
% \thispagestyle{empty}

\pagestyle{mine}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Chapters
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\cleardoublepage{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
\label{chapter:Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Amidst the rapid advancement of deep neural networks (DNNs), the field of music information retrieval (MIR) has witnessed a significant shift from traditional heuristic and mathematical models to data-driven approaches that heavily rely on large amounts of labelled training data, such as pitch estimation \cite{Kim2018_CREPEmodel_ISMIR}, audio embeddings \cite{Cramer2019_OpenL3_ISMIR}, automatic music transcription \cite{Benetos2019_AutomaticMusicTranscription}.

However, the reliance on large and accurate datasets poses many challenges, considering the time-consuming and labor-intensive nature of manual annotation, as well as the potential for human error and subjectivity. Thus, it is generally difficult to obtain strongly aligned annotations, where each frame of the audio signal is associated with a corresponding label. Instead, weakly aligned or unaligned annotations are more common, where only the presence or absence of certain labels is known, without precise temporal alignment. This ease up the data acquisition process, but requires a more sophisticated loss function to train the model. One proposed solution is using connectionist temporal classification (CTC) loss \cite{Graves2006_ConnectionistTemporalClassification_ICML}.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Soft Dynamic Time Warping Algorithm}
\label{chapter:sdtw}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In this chapter, we present the mathematical formulation of the soft-DTW algorithm.
\section{Definition and Notation}
Let $\mathbf{x} = (x_0, \ldots, x_{N-1}) \in \mathbb{R}^N$  and $\mathbf{y} = (y_0, \ldots, y_{M-1}) \in \mathbb{R}^M$ be two time series of representing the sequence of predictions and strong targets, respectively. We then denote the soft target sequence as $\mathbf{y'} = (y'_0, \ldots, y'_{M'-1}) \in \mathbb{R}^{M'}$, where $M' < M$. The objective of soft-DTW is to calculate the alignment cost matrix between $\mathbf{x}$ and $\mathbf{y'}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Forward Pass}
\label{section:forward_pass}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In the forward pass, we compute the accumulated cost matrix $\mathbf{D}(M,N) \in \mathbb{R}^{M \times N}$, where each element $D(i,j)$ represents the minimum cost of aligning the first $i$ elements of $\mathbf{y'}$ with the first $j$ elements of $\mathbf{x}$.
The accumulated cost is computed using the local cost matrix $\mathbf{C}$, where $\mathbf{C}(i,j) = c(x_i, y'_j)$, which measures the dissimilarity between the elements $x_i$ and $y'_j$. A common choice for the local cost function is the squared Euclidean distance:
\begin{equation}
    \mathbf{C}(i,j) = \| x_i - y'_j \|^2
\end{equation}
Instead of using the hard minimum operator as in traditional DTW, soft-DTW employs a differentiable approximation, defined as:
\begin{equation}
    \text{min}^\gamma = -\gamma \log \sum_{s \in S} \left( e^{-s/\gamma} \right)
\end{equation}
where $\gamma > 0$ is a smoothing parameter that controls the softness of the minimum operation, and $S$ is the set of values over which the soft minimum is computed.
Combining with the step constraint of DTW, we only consider three possible predecessor cells for each cell $(i,j)$, which are $(i-1,j)$, $(i,j-1)$, and $(i-1,j-1)$.
Thus the accumulated cost matrix $\mathbf{D}$ is computed recursively as follows:
\begin{equation}
    \mathbf{D}(i,j) = \mathbf{C}(i,j) + \text{min}^\gamma \left( \mathbf{D}(i-1,j), \mathbf{D}(i,j-1), \mathbf{D}(i-1,j-1) \right)
\end{equation}

The recursive algorithm is summarized in Algorithm \ref{algorithm:forward_pass}.

\begin{algorithm}[t]
    \caption{Forward Pass of Soft-DTW}
    \label{algorithm:forward_pass}
    \begin{algorithmic}
    \Require Time series $\mathbf{x} \in \mathbb{R}^N$, $\mathbf{y'} \in \mathbb{R}^{M'}$, smoothing parameter $\gamma > 0$
    \Ensure Accumulated cost matrix $\mathbf{D} \in \mathbb{R}^{M' \times N}$
    \State Initialize $\mathbf{D}(0,0) = 0$
    \For{$i = 1$ to $M'$}
        \State $\mathbf{D}(i,0) = \infty$
    \EndFor
    \For{$j = 1$ to $N$}
        \State $\mathbf{D}(0,j) = \infty$
    \EndFor
    \For{$i = 1$ to $M'$}
        \For{$j = 1$ to $N$}
            \State Compute local cost: $\mathbf{C}(i,j) = \| x_j - y'_i \|^2$
            \State Update accumulated cost:
            \[
            \mathbf{D}(i,j) = \mathbf{C}(i,j) + \text{min}^\gamma \left( \mathbf{D}(i-1,j), \mathbf{D}(i,j-1), \mathbf{D}(i-1,j-1) \right)
            \]
        \EndFor
    \EndFor

    \Return $\mathbf{D}$
    \end{algorithmic}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Backward Pass}
\label{section:backward_pass}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In order to train the model, we need to compute the gradients of the soft-DTW loss with respect to the model parameters. The backward pass computes the gradient $H \in \mathbb{R}^{M' \times N}$ of the soft-DTW cost with respect to the cost matrix $\mathbf{C} \in \mathbb{R}^{M' \times N}$.
An efficient way to compute the gradient is to use a dynamic programming approach similar to the forward pass. The gradient is computed recursively as follows:
\begin{equation}
    \mathbf{H}(i,j) = \frac{\partial \mathbf{D}(M',N)}{\partial \mathbf{D}(i,j)} \frac {\partial \mathbf{D}(i,j)}{\partial \mathbf{C}(i,j)}
\end{equation}
During the backward pass, we consider three predecessor cells for each cell $(i,j)$, which are $(i+1,j)$, $(i,j+1)$, and $(i+1,j+1)$.
This expands the gradient computation to:
\begin{equation}
    \mathbf{H}(i,j) = \frac{\partial \mathbf{D}(M',N)}{\partial \mathbf{D}(i+1,j)} \frac{\partial \mathbf{D}(i+1,j)}{\partial \mathbf{D}(i,j)} + \frac{\partial \mathbf{D}(M',N)}{\partial \mathbf{D}(i,j+1)} \frac{\partial \mathbf{D}(i,j+1)}{\partial \mathbf{D}(i,j)} + \frac{\partial \mathbf{D}(M',N)}{\partial \mathbf{D}(i+1,j+1)} \frac{\partial \mathbf{D}(i+1,j+1)}{\partial \mathbf{D}(i,j)}
\end{equation}

For simplicity, we denote $\mathbf{E}(i,j) = \frac{\partial \mathbf{D}(M',N)}{\partial \mathbf{D}(i,j)}$ and $\mathbf{F}^{p, q}(i,j) = \frac{\partial \mathbf{D}(i+p,j+q)}{\partial \mathbf{D}(i,j)}$, where $p, q \in \{0, 1\}$.

Using the derivative of the soft minimum operator w.r.t. to its input $s \in \mathcal S$
\begin{equation}
    \frac{\partial \text{min}^\gamma(s)}{\partial s} = \frac{e^{-s/\gamma}}{\sum_{s' \in \mathcal S} e^{-s'/\gamma}}
\end{equation}

Without loss of generality, we can compute $\mathbf{F}^{1,0}(i,j)$ as follows:
\begin{equation}
    \mathbf{F}^{1,0}(i,j) = \frac{\partial \text{min}^\gamma(\mathbf{D}(i+1,j))}{\partial \mathbf{D}(i,j)} \\
    = \frac{e^{-(\mathbf{C}(i+1,j) - \mathbf{D}(i,j))/\gamma}}{e^{-\mathbf{D}(i,j)/\gamma}}
\end{equation}
Similarly, we can compute $\mathbf{F}^{0,1}(i,j)$ and $\mathbf{F}^{1,1}(i,j)$.
Then we can reformulate $\mathbf{E}$ as:
\begin{equation}
    \mathbf{E}(i,j) = \mathbf{E}(i+1,j) \mathbf{F}^{1,0}(i,j) + \mathbf{E}(i,j+1) \mathbf{F}^{0,1}(i,j) + \mathbf{E}(i+1,j+1) \mathbf{F}^{1,1}(i,j)
\end{equation}
Denote $\mathbf{G}(i,j) = \frac{\partial \mathbf{D}(i,j)}{\partial \mathbf{C}(i,j)}$, we have the final formulation of $\mathbf{H}$ as:
\begin{equation}
    \mathbf{H}(i,j) = \mathbf{E}(i,j) \mathbf{G}(i,j)
\end{equation}
For initialization, we add and extra column and row to $\mathbf{E}$, setting $\mathbf{E}(M'+1, N+1) = 1$ and $\mathbf{E}(i,i)$ to be $-\infty$ and $0$ for the rest of the extra row and column. Algorithm \ref{algorithm:backward_pass} presents the complete overview of the recursion for gradient computation.

\begin{algorithm}[t]
    \caption{Backward Pass of Soft-DTW}
    \label{algorithm:backward_pass}
    \begin{algorithmic}
        \Require $\mathbf{C}, \mathbf{D} \in \mathbb{R}^{M' \times N}$, $\gamma > 0$
        \Ensure Gradient matrix $\mathbf{H} \in \mathbb{R}^{M' \times N}$
        \State $\mathbf{C}(M'+1,:) = \mathbf{C}(:,N+1) = 0$
        \State $\mathbf{D}(M'+1,:) = \mathbf{D}(:,N+1) = -\infty$
        \State $\mathbf{D}(M'+1,N+1) = D(M',N)$
        \State $\mathbf{E}(M'+1,:) = \mathbf{E}(:,N+1) = 0$
        \State $\mathbf{E}(M'+1,N+1) = 1$
        \For {i = M' down to 1}
            \For {j = N down to 1}
                \State Compute $\mathbf{F}^{1,0}(i,j), \mathbf{F}^{0,1}(i,j), \mathbf{F}^{1,1}(i,j)$
                \State Update $\mathbf{E}(i,j) = \mathbf{E}(i+1,j) \mathbf{F}^{1,0}(i,j) + \mathbf{E}(i,j+1) \mathbf{F}^{0,1}(i,j) + \mathbf{E}(i+1,j+1) \mathbf{F}^{1,1}(i,j)$
                \State Compute $\mathbf{G}(i,j) = \frac{\partial \mathbf{D}(i,j)}{\partial \mathbf{C}(i,j)}$
                \State Update $\mathbf{H}(i,j) = \mathbf{E}(i,j) \mathbf{G}(i,j)$
            \EndFor 
        \EndFor

        \Return $\mathbf{H}$
    \end{algorithmic}
\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Experimental Setup}
\label{chapter:experimental_setup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In this section, we present the objectives of our experiment, the dataset used for training alongside with the network architecture and the training process.

\section{Dataset}
\label{section:dataset}
For the experiment, we use the Beatles dataset retrieved from Isophonics \cite{HarteSAG05_ChordRecognition_ISMIR}, consisting of four audio recordings with respective annotations. 
Since the original annotations have more than 24 chord types, which would make the network too complex and beyond the scope of this project.
We therefore consider the simplified version of such annotations, which reduced the number of chord types to only 24 (12 chromas with their respective major or minor variant)\cite{Mueller15_FundamentalsMusicProcessig_SPRINGER}.
We split the dataset into training, validation, and test sets. For test set, a short segment of Let It Be is used,
while the rest of the dataset is split into 3:1 ratio for training and validation.

We choose a sequence length of 150 samples for training and validating the model, creating 43 and 12 segments for training and validation, respectively.
In case of soft alignment, we remove the adjacent repetitions in the sequence, effectively reducing its length by around 85\% on average (see Figure \ref{figure:soft_alignment}).
\begin{figure}[t]
    \centering
    \includegraphics[scale=0.5]{figures/data_strong.png}
    \\
    \vspace{0.5cm}
    \includegraphics[scale=0.5]{figures/data_soft.png}
    \caption{Example of strong and soft target sequences. The strong target sequence has a length of 150, while the soft target sequence has a length of 5 after removing adjacent repetitions.}
    \label{figure:soft_alignment}
\end{figure}

However, this method introduce a problem where batching target sequences of different lengths is not possible. To address this issue, after reduction, we pad the sequences repeating elements until they reach a desired length, or "soft length".
After some experiments, we found that a soft length of 16 covers all of possible reduced sequences while keeping the reduction ratio high. For which elements are repeated, we derived several strategies, either uniformly over all elements, or only repeat the last element until reaching the desired length.


\section{Model Architecture}
\label{section:model_architecture}
Given the aim of this experiment is to evaluate the performance of the proposed SDTW loss function, the network architecture plays a minor role and are kept simple.
Therefore, we used a simple chord recognition network (dChord) that based on the template-based chord recognition algorithm.
This network consists of a single layer that acts as the chord template to predict a 24-dimensional activation vector, corresponding to 24 chord types.
The network has a total of 25 trainable parameters.
Table \ref{table:arch} illustrate the components of the architecture with their respective input and output dimensions.

\begin{table}[t]
    \begin{center}
    \begin{tabular}{|c|c|c|c|}
        \hline
        Layer & Input Dimension & Output Dimension & Parameters \\
        \hline
        Log-compression & (12, 150) & (12, 150) & 1 \\
        Normalization & (12, 150) & (12, 150) & 0 \\
        dChord & (12, 150) & (24, 150) & 24 \\
        softmax & (24, 150) & (24, 150) & 0 \\
        \hline
    \end{tabular}
        \caption{Architecture of the chord recognition network. T is the number of time frames.}
        \label{table:arch}
    \end{center}
\end{table}

During the training process, we use Adam Optimizer \cite{Kingma2017_AdamMethodStochasticOptimization} with a a learning rate $\alpha = 0.1$ with $\beta_1 = 0.9$, $\beta_2 = 0.999$ and $\epsilon = 10^{-8}$. Since the dataset is small, we use a batch size of 5 and enable share weight, where the weight of chords in the same class (major or minor) are shared. The model is trained for 200 epochs, and the best model is selected based on the validation loss.



\chapter{Evaluation}
\label{chapter:evaluation}
In this section, we evaluate the performance of the aforementioned soft-DTW loss function against the baseline model trained with binary cross-entropy loss for the task of chord recognition. For each case, we train with the respective dataset, either strong or soft targets, and calculate the F1 measure on the test set.

\section{Training with SDTW loss}
\label{section:training_sdtw}
Figure \ref{figure:train_curves} shows the training and validation loss curves of the models trained on strong and soft targets. We can see overall the loss decreases over time, indicating that the models are learning from the data,
despite much higher loss value for the model trained on soft target, which ranges from 130 to 70, compared to 2.5 to 0.75 for the model trained on strong target.This is expected, as the soft target contains less information, making it more difficult for the model to learn.

\begin{figure}[t]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[scale=0.5]{figures/results_strong/learncurve-strong.png}
        \caption{Training and validation loss curves of the model trained on strong target with binary cross-entropy loss. The model converges after around 20 epochs.}
        \label{figure:strong_train_curves}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[scale=0.5]{figures/results_soft/learncurve-gam03.png}
        \caption{Training and validation loss curves of the model trained on soft target with soft-DTW loss. The model converges after around 60 epochs.}
        \label{figure:soft_train_curves}
    \end{subfigure}
    \caption{Training and validation loss curves of the models trained on strong and soft targets.}
    \label{figure:train_curves}
\end{figure}

\section{Training results}
\label{section:baseline}

\begin{table}[t]
    \begin{center}
        \begin{tabular}{|c|c|c|c|}
            \hline
            Target/ Original Length & Loss Function & Test F1-score (\%)    & Epochs \\
                                    & (Pad Strategy) &                      & to best val. loss \\
            \hline
            150/150 & Binary Cross-entropy & 0.765 & 50 \\
            \hline
            16/150 & Soft-DTW (uniform) & 0.761 & 200 \\
            75/150 & Soft-DTW (uniform) & 0.761 & 200 \\
            \hline
            16/150 & Soft-DTW (last) & 0.761 & 200 \\
            75/150 & Soft-DTW (last) & 0.681 & 200 \\
            \hline
        \end{tabular}
    \end{center}
    \caption{Results of the baseline model trained on strong target}
    \label{table:results}
\end{table}
Table \ref{table:results} summarizes the results of the baseline model trained on strong target with binary cross-entropy loss and the model trained on soft target with soft-DTW loss. Here we choose two different soft lengths, 16 and 75 to evaluate the effect of the reduction ratio on the performance. We also compare two different padding strategies, uniform and last element repetition.

Overall, we see that sDTW loss is able to achieve comparable performance to the baseline model trained on strong target, with a slight decrease in F1-score of 0.4\%.
This shows that sDTW is able to effectively learn from the soft target that contains less or noisy information,
making it more robust to variations in the input sequence.
Observing the results of different padding strategy, we find that last element repetition leads to similar performance as
uniform padding for short soft lengths though it eventually degrades as the soft length increases. This is likely due to the repetition,
which introduces a bias towards the last element, making it more prominent the longer the soft length. Figure \ref{figure:padding_strategy}
shows different training target sequences with respective padding strategy and soft length. We can see that uniform padding distributes the repetitions evenly across all elements,
while last element repetition creates a long tail of the last element. We speculate this would cause the model to overfit to the last element, as it becomes more frequent in the training data;
therefore, it undermines the model's ability to generalize even with sDTW loss. Nonetheless, it also shows that sDTW are rather flexible, being able to compensate for small
mistakes in annotation. And it would perform just as well for longer length had the padding was uniform.

\begin{figure}
    \centering
    \begin{subfigure}{0.48\textwidth}
        \centering
        % \includegraphics[scale=0.5]{figures/data_soft16_uniform.png}
        \caption{Uniform padding with soft length of 16.}
        \label{figure:uniform_16}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \centering
        % \includegraphics[scale=0.5]{figures/data_soft16_last.png}
        \caption{Last element repetition with soft length of 16.}
        \label{figure:last_16}
    \end{subfigure}
    \\
    \vspace{0.5cm}
    \begin{subfigure}{0.48\textwidth}
        \centering
        % \includegraphics[scale=0.5]{figures/data_soft75_uniform.png}
        \caption{Uniform padding with soft length of 75.}
        \label{figure:uniform_75}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \centering
        % \includegraphics[scale=0.5]{figures/data_soft75_last.png}
        \caption{Last element repetition with soft length of 75.}
        \label{figure:last_75}
    \end{subfigure}
    \caption{Example of different padding strategies with different soft lengths. The original strong target sequence has a length of 150, while the soft target sequence has a length of 5 after removing adjacent repetitions. The padded sequences have lengths of 16 and 75, respectively.}
    \label{figure:padding_strategy}
\end{figure}

\section{Model Characteristics}
\label{section:model_characteristics}
In order to understand the behavior of the model trained with soft targets, we analyze the weight distribution of the model parameters. Specifically, we examine the distribution of the weights in the final layer of the model, which is responsible for making the predictions. We also observe the computed gradients during training to see how the model is learning from the data.

Figure \ref{figure:weight_histogram} shows the weight distribution of the final layer over two chord classes. As mentioned before, we enable weight sharing across twelve chromas to counteract the small dataset size, hence the weight distribution appears similar to a transposition of a single chord. It is apparent that the final weight of the major chord class is more distinguished and vivid than that of the minor chord class, which is more faint, indicating less weight.
This is mainly due to class imbalance presents in our dataset.
Taking a closer look in Table \ref{table:chord_class_distribution}, we can see that the major chord class is more prevalent in the dataset, taking up 91\% of the total chord contained in strong target case.

In the case of soft targets, we observe a less pronounced class imbalance, with the minor chords now taking up to 12\% of the total chord in soft target case. Further investigation with different soft lengths using uniform padding strategy shows that the class frequency fluctuates with respect to the soft length, albeit shows a more balanced distribution compared to the strong target case as shown in Table \ref{table:chord_class_distribution_soft}.
\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/results_soft/weight-gam03.png}
    \caption{Weight distribution of the final layer over two chord classes. Higher values indicate larger weight.}
    \label{figure:weight_histogram}
\end{figure}

\begin{table}
    \begin{center}
        \begin{tabular}{|c|c|c|c|c|c|}
            \hline
            Chord Class & Number of Samples & Percentage        & Number of Samples & Percentage\\
                        &   (strong)        &   (strong)        &   (soft length 16)          &   (soft length 16)\\
            \hline
            Major       & 5843    & 90.59\%  & 613 & 89.10\%\\
            Minor       & 607     & 9.41\% & 75 & 10.90\%\\
            No Chord    & 0       & 0\% & 0 & 0\%\\
            \hline
            \textbf{Total}       & 6450    & 100\% & 688 & 100\%\\
            \hline
        \end{tabular}

    \end{center}
    \caption{Distribution of chord classes in the dataset.}
    \label{table:chord_class_distribution}
\end{table}

\begin{table}
    \begin{center}
        \begin{tabular}{|c|c|c|c|c|c|c|}
            \hline
            \multirow{2}{*}{Chord Class} & \multicolumn{2}{c|}{Soft "0"} & \multicolumn{2}{c|}{Soft 16} & \multicolumn{2}{c|}{Soft 75}\\
            \cline{2-7}
                    & Samples & \% & Samples & \% & Samples & \%\\
            \hline
            Major       & 316    & 87.78\%  & 613 & 89.10\% & 2656 & 88.53\%\\
            Minor       & 44     & 12.22\% & 75 & 10.90\% & 344 & 11.47\%\\
            No Chord    & 0       & 0\% & 0 & 0\% & 0 & 0\%\\
            \hline
            \textbf{Total}       & 360    & 100\% & 688 & 100\% & 3000 & 100\%\\
            \hline
        \end{tabular}
    \end{center}
    \caption{Distribution of chord classes in the dataset with different soft lengths. "0" refers to no padding. Padding strategy is uniform repetition.}
    \label{table:chord_class_distribution_soft}
\end{table}

Looking at the computed gradients during training shown in Figure \ref{figure:gradient_over_epochs_uniform} and \ref{figure:gardient over_epochs_repetition}, we can observe the probability distribution of the alignment path.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusions}
\label{chapter:conclusions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In this report, we have presented a comprehensive study on the use of soft targets for chord recognition in music audio. Our experiments demonstrate that leveraging soft targets can significantly improve the performance of chord recognition models, particularly in the presence of class imbalance.

We have shown that the model trained with soft targets is able to better generalize to unseen data, as evidenced by the improved performance metrics on the validation set. The use of soft targets allows the model to learn from a richer set of annotations, capturing the nuances of chord progressions more effectively.

Furthermore, our analysis of the model's behavior reveals interesting insights into the learning process. The weight distributions and gradient flows indicate that the model is able to focus on the most relevant features for chord recognition, leading to more robust predictions.

Overall, our findings suggest that incorporating soft targets into the training process is a promising approach for addressing the challenges of chord recognition in music audio.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % Appendix
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \cleardoublepage{}
% \appendix
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \chapter{Source Code}
% \label{chapter:source_code}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% In this chapter, the headers of selected \MATLAB{} functions created during the writing of this thesis are reproduced. The headers contain information about the name of the described function and its input/output behavior.

% \section*{Feature Extraction}
% The \texttt{file\_to\_feature} function is used as a wrapper for several low-level functions that perform feature extraction or loading of precomputed features.

% Sample usage:\\
% \scriptsize
% \verb|[f_pitch, f_peaks] = file_to_feature('features', 'pathetique.wav');|

% \begin{verbatim}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % Name: file_to_feature
% % Version: 1.0
% % Date: 11.05.2010
% % Programmer: John Q. Public
% %
% % Description:
% %   Load or compute features for audio and MIDI files
% %
% % Input:
% % - dirname: Directory where the file or features are located
% % - filename: Name of the file for which to load/compute features
% % - parameter
% %            .win_len: Window length used for STMSP feature generation
% %            .win_res: Window resolution
% %
% % Output:
% % - f_pitch: Pitch features (STMSP)
% % - f_peaks: Energy peaks for onset computation
% % - f_onsets: Precise onsets (only generated in case of MIDI input data)
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \end{verbatim}
% \normalsize

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Bibliography
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\cleardoublepage{}
{
\small
\addcontentsline{toc}{chapter}{Bibliography}
\bibliographystyle{ieeetr}
\bibliography{bibliography}
}
\cleardoublepage{}


\end{document}
