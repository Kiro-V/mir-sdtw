{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d44f8960-8c38-4c82-936f-98d0a21f12f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T15:02:52.938981Z",
     "start_time": "2025-07-03T15:02:35.180807100Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import librosa\n",
    "\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "import libfmp.b\n",
    "import libfmp.c3\n",
    "import libfmp.c4\n",
    "import libfmp.c5\n",
    "%matplotlib inline\n",
    "\n",
    "data_basedir = os.path.join('.', 'data', 'dChord')\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52467d29-0db7-4f95-b7f2-d63d42d9e744",
   "metadata": {},
   "source": [
    "## Network configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1b01ba9-c92a-4dd6-91c7-6c28c20de456",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T15:13:33.823919700Z",
     "start_time": "2025-07-03T15:13:33.771060300Z"
    }
   },
   "outputs": [],
   "source": [
    "# code copied from libdl.nn_models.cnns_chord_recog\n",
    "\n",
    "class log_compression(torch.nn.Module):\n",
    "    \"\"\"Module for logarithmic compression of an array\n",
    "\n",
    "    Args:\n",
    "        gamma: Compression factor\n",
    "        trainable: Whether the gradient w.r.t. gamma is computed in backward pass\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma_init=1, trainable=True):\n",
    "        super(log_compression, self).__init__()\n",
    "\n",
    "        # define logarithm of gamma as trainable parameter\n",
    "        if gamma_init is not None:\n",
    "            self.log_gamma = torch.nn.parameter.Parameter(data=torch.log(torch.as_tensor(gamma_init, dtype=torch.float32)), requires_grad=trainable)\n",
    "        else:\n",
    "            self.log_gamma = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.log_gamma is not None:\n",
    "            return torch.log(1.0 + torch.exp(self.log_gamma) * x)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "class gaussian_filter(torch.nn.Module):\n",
    "    \"\"\"Module for generating a 1D Gaussian filter\n",
    "\n",
    "    Args:\n",
    "        length: kernel length\n",
    "        sigma: (initial) standard deviation for Gaussian kernel\n",
    "        dim: dimension across which to apply the filter; output tensor will have singleton dimension at 'dim'\n",
    "        trainable: whether to optimize the standard deviation\n",
    "    \"\"\"\n",
    "    def __init__(self, length=41, sigma_init=1, dim=2, trainable=True):\n",
    "        super(gaussian_filter, self).__init__()\n",
    "\n",
    "        self.length = length\n",
    "        self.dim = dim\n",
    "\n",
    "        if sigma_init is None:\n",
    "            sigma_init = length\n",
    "        self.log_sigma = torch.nn.parameter.Parameter(data=torch.log(torch.as_tensor(sigma_init, dtype=torch.float32)), requires_grad=trainable)\n",
    "\n",
    "    def forward(self, x):\n",
    "        idx = x.ndim * [1]\n",
    "        idx[self.dim] = self.length\n",
    "        w = self.get_kernel()\n",
    "        x_smoothed = torch.sum(x * w.view(*idx), dim=self.dim, keepdim=True)\n",
    "        return x_smoothed\n",
    "\n",
    "    def get_kernel(self):\n",
    "        n = torch.arange(0, self.length).to(self.log_sigma.device) - (self.length - 1.0) / 2.0\n",
    "        sig2 = 2 * torch.exp(self.log_sigma) ** 2\n",
    "        w = torch.exp(-n ** 2 / sig2)\n",
    "        return w / torch.sum(w)\n",
    "\n",
    "\n",
    "class temporal_smoothing(torch.nn.Module):\n",
    "    \"\"\"Module for temporal smoothing of a feature sequence.\n",
    "\n",
    "    Args:\n",
    "        smoothing_type: Either 'weighted_sum', 'median' or 'Gaussian'\n",
    "        avg_length: Length to be averaged over; only relevant for 'weighted_sum' and 'Gaussian' (median is taken over whole input length)\n",
    "        weight_init: How to initialize (trainable) weights, only relevant for 'weighted_sum' (either 'uniform' or 'random')\n",
    "    \"\"\"\n",
    "    def __init__(self, smoothing_type='weighted_sum', avg_length=41, weight_init='uniform', sigma_init=20):\n",
    "        super(temporal_smoothing, self).__init__()\n",
    "        if smoothing_type not in {'weighted_sum', 'median', 'Gaussian'}:\n",
    "            raise ValueError('Smoothing type ' + smoothing_type + ' is unknown!')\n",
    "\n",
    "        if weight_init not in {'random', 'uniform'}:\n",
    "            raise ValueError('Weight initialization ' + weight_init + ' is unknown!')\n",
    "\n",
    "        self.avg_length = avg_length\n",
    "\n",
    "        if smoothing_type == 'weighted_sum':\n",
    "            self.filter = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(avg_length, 1),\n",
    "                                    stride=(1, 1), bias=False, padding='valid')\n",
    "\n",
    "            if weight_init == 'random':\n",
    "                pass\n",
    "            elif weight_init == 'uniform':\n",
    "                self.filter.weight.data = torch.ones_like(self.filter.weight.data) / avg_length\n",
    "\n",
    "        elif smoothing_type == 'median':\n",
    "            self.filter = lambda x: torch.median(x, dim=2, keepdim=True,).values\n",
    "\n",
    "        elif smoothing_type == 'Gaussian':\n",
    "            self.filter = gaussian_filter(length=avg_length, sigma_init=sigma_init, dim=2, trainable=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # treat channels as batch dimension to use conv. layer with 1 channel\n",
    "        x_reshaped = x.view(-1, 1, *x.shape[2:])\n",
    "        x_filtered = self.filter(x_reshaped)\n",
    "        return x_filtered.view(*x.shape[:2], *x_filtered.shape[2:])\n",
    "\n",
    "\n",
    "class feature_normalization(torch.nn.Module):\n",
    "    \"\"\"Module for feature normalization\n",
    "\n",
    "    Args:\n",
    "        num_features: Number of features\n",
    "        norm: The norm to be applied. '1', '2', 'max' or 'z'\n",
    "        threshold: Threshold below which the vector `v` is used instead of normalization\n",
    "        v: Used instead of normalization below `threshold`. If None, uses unit vector for given norm\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features=12, norm='2', threshold=1e-4, v=None, dim=3):\n",
    "        super(feature_normalization, self).__init__()\n",
    "        if norm not in ['1', '2', 'max', 'z']:\n",
    "            raise ValueError('Norm ' + norm + ' is unknown!')\n",
    "\n",
    "        self.threshold = threshold\n",
    "        self.v = v\n",
    "\n",
    "        if norm == '1':\n",
    "            if self.v is None:\n",
    "                self.v = torch.ones(num_features, dtype=torch.float32) / num_features\n",
    "            self.get_norms = lambda x: torch.linalg.vector_norm(x, ord=1.0, dim=dim, keepdim=False)\n",
    "            self.normalize = lambda x: F.normalize(x, p=1.0, dim=dim, eps=threshold)\n",
    "\n",
    "        if norm == '2':\n",
    "            if self.v is None:\n",
    "                self.v = torch.ones(num_features, dtype=torch.float32) / torch.sqrt(torch.tensor([num_features]))\n",
    "            self.get_norms = lambda x: torch.linalg.vector_norm(x, ord=2.0, dim=dim, keepdim=False)\n",
    "            self.normalize = lambda x: F.normalize(x, p=2.0, dim=dim, eps=threshold)\n",
    "\n",
    "        if norm == 'max':\n",
    "            if self.v is None:\n",
    "                self.v = torch.ones(num_features, dtype=torch.float32)\n",
    "            self.get_norms = lambda x: torch.linalg.vector_norm(x, ord=float('inf'), dim=dim, keepdim=False)\n",
    "            self.normalize = lambda x: F.normalize(x, p=float('inf'), dim=dim, eps=threshold)\n",
    "\n",
    "        if norm == 'z':\n",
    "            if self.v is None:\n",
    "                self.v = torch.zeros(num_features)\n",
    "            self.get_norms = lambda x: torch.std(x, dim=dim, keepdim=False, unbiased=True)\n",
    "            self.normalize = lambda x: (x - torch.mean(x, dim=dim, keepdim=True)) / torch.std(x, dim=dim, keepdim=True,\n",
    "                                                                                              unbiased=True)\n",
    "    def forward(self, x):\n",
    "        x_norms = self.get_norms(x)\n",
    "        idx = x_norms > self.threshold\n",
    "        x_normalized = x.clone()\n",
    "        x_normalized = self.normalize(x_normalized)\n",
    "        x_normalized[~idx] = self.v.to(x.device)\n",
    "        return x_normalized\n",
    "\n",
    "\n",
    "class chord_recognition_templates(torch.nn.Module):\n",
    "    \"\"\"Module for applying template-based chord recognition to chroma features.\n",
    "\n",
    "    Args:\n",
    "        shared_weights: Whether to use 2 shared kernels (maj/min) or 24 individual chord templates\n",
    "        initialize_parameters: Whether to initialize kernels with idealized binary chord templates and zero bias\n",
    "        normalize_weights: Whether to normalize all templates to unit Euclidean norm\n",
    "        bias: Whether to allow for a trainable bias\n",
    "    \"\"\"\n",
    "    def __init__(self, shared_weights=True, initialize_parameters=True, normalize_weights=True, bias=False):\n",
    "        super(chord_recognition_templates, self).__init__()\n",
    "        self.include_blank = include_blank\n",
    "\n",
    "        if shared_weights:\n",
    "            self.padding = lambda x: F.pad(x, pad=(0, 11, 0, 0), mode='circular')\n",
    "            self.filter = torch.nn.Conv2d(in_channels=1, out_channels=3 if include_blank else 2, kernel_size=(1, 12), stride=(1, 1), bias=bias)\n",
    "            if initialize_parameters:\n",
    "                self.filter.weight.data = torch.zeros_like(self.filter.weight.data)\n",
    "                self.filter.weight.data[0, 0, 0, :] = torch.tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0], dtype=torch.float32) # major\n",
    "                self.filter.weight.data[1, 0, 0, :] = torch.tensor([1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0], dtype=torch.float32) # minor\n",
    "                # self.filter.weight.data = torch.tensor([[[[1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0]]],     # major\n",
    "                #                                         [[[1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0]]]],    # minor\n",
    "                                                       # dtype=torch.float32)\n",
    "                if include_blank:\n",
    "                    self.filter.weight.data[2, 0, 0, :] = torch.ones(12)/12\n",
    "                if self.filter.bias is not None:\n",
    "                    self.filter.bias.data = torch.zeros_like(self.filter.bias.data, dtype=torch.float32)\n",
    "\n",
    "        else:\n",
    "            self.padding = lambda x: x\n",
    "            self.filter = torch.nn.Conv2d(in_channels=1, out_channels=25 if include_blank else 24, kernel_size=(1, 12), stride=(1, 1), bias=bias)\n",
    "            if initialize_parameters:\n",
    "                kernel_major = torch.tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0])\n",
    "                kernel_minor = torch.tensor([1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0])\n",
    "\n",
    "                for i in range(12):\n",
    "                    self.filter.weight.data[i, 0, 0, :] = torch.roll(kernel_major, i)\n",
    "                    self.filter.weight.data[12 + i, 0, 0, :] = torch.roll(kernel_minor, i)\n",
    "                if include_blank:\n",
    "                    self.filter.weight.data[24, 0, 0, :] = torch.ones(12) / 12\n",
    "\n",
    "                if self.filter.bias is not None:\n",
    "                    self.filter.bias.data = torch.zeros_like(self.filter.bias.data, dtype=torch.float32)\n",
    "\n",
    "        if normalize_weights:\n",
    "            self.filter.weight.data = F.normalize(self.filter.weight.data, p=2.0, dim=3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_padded = self.padding(x)                                          # out: (B x 1 x T x 23) or (B x 1 x T x 12)\n",
    "        y = self.filter(x_padded)                                           # out: (B x 3 x T x 12) or (B x 25 x T x 1)\n",
    "        if self.include_blank and shared_weights:\n",
    "            blank_channel = y[:, 2:3, :, :]\n",
    "            y = y[:, :2, :, :]\n",
    "            y_reshaped = torch.swapaxes(y, 1, 2)                            # out: (B x T x 2 x 12)\n",
    "            y_reshaped = torch.flatten(y_reshaped, start_dim=2)             # out: (B x T x 24)\n",
    "            blank_replicaed = blank_channel.squeeze(1).repeat(1, 1, 1, 12)  # out: (B x T x 1 x 12)\n",
    "            blank_flatten = torch.flatten(blank_replicaed, start_dim=2)     # out: (B x T x 12)\n",
    "            y_reshaped = torch.cat([y_reshaped, blank_flatened], dim=2)     # out: (B x T x 25)\n",
    "            y_reshaped = torch.unsqueeze(y_reshaped, 1)                     # out: (B x 1 x T x 25)\n",
    "        else:\n",
    "            y_reshaped = torch.swapaxes(y, 1, 2)                                # out: (B x T x 2 x 12) or (B x T x 24 x 1)\n",
    "            y_reshaped = torch.unsqueeze(torch.flatten(y_reshaped, start_dim=2), 1)             # out: (B x 1 x T x 24)\n",
    "        return y_reshaped\n",
    "\n",
    "\n",
    "class softmax_temperature(torch.nn.Module):\n",
    "    \"\"\"Softmax activation with trainable temperature parameter\n",
    "\n",
    "    Args:\n",
    "        dim: Dimension across which to apply the softmax function\n",
    "        tau: Temperature parameter\n",
    "        trainable: Whether the gradient w.r.t. tau is computed in backward pass\n",
    "    \"\"\"\n",
    "    def __init__(self, dim=3, tau=1, trainable=True):\n",
    "        super(softmax_temperature, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.tau = torch.nn.parameter.Parameter(data=torch.tensor([tau], dtype=torch.float32), requires_grad=trainable)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.softmax(x / self.tau, dim=self.dim)\n",
    "\n",
    "\n",
    "class chord_recog_templates_pipeline(torch.nn.Module):\n",
    "    \"\"\"Model for template-based chord recognition:\n",
    "\n",
    "    Pipeline:\n",
    "        1) input: chroma features\n",
    "        2) log compression (gamma trainable)\n",
    "        3) temporal averaging (conv. layer with trainable weights / median filter / Gaussian filter)\n",
    "        4) l1/l2/max/z normalization (only if norm large enough; otherwise: unit-norm vector with same values)\n",
    "        5) chord recognition via templates (conv. layer)\n",
    "        6) softmax -> output: chord probabilities\n",
    "\n",
    "    Args:\n",
    "        dictionaries containing parameters for the individual building blocks\n",
    "    \"\"\"\n",
    "    def __init__(self, compression_params=None, temp_smooth_params=None, feature_norm_params=None,\n",
    "                 chord_template_params=None, softmax_params=None):\n",
    "\n",
    "        super(chord_recog_templates_pipeline, self).__init__()\n",
    "\n",
    "        self.log_compression = log_compression(**compression_params)\n",
    "        self.temporal_smoothing = temporal_smoothing(**temp_smooth_params)\n",
    "        self.feature_normalization = feature_normalization(**feature_norm_params)\n",
    "        self.chord_recognition_templates = chord_recognition_templates(**chord_template_params)\n",
    "        self.softmax_temperature = softmax_temperature(**softmax_params)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_pred, _, _, _, _ = self.get_intermediate_data(x)\n",
    "        return y_pred\n",
    "\n",
    "    def get_intermediate_data(self, x):\n",
    "        x_comp = self.log_compression(x)                                    # out: (B x 1 x T x 12)\n",
    "        x_avg = self.temporal_smoothing(x_comp)                             # out: (B x 1 x 1 x 12)\n",
    "        x_normalized = self.feature_normalization(x_avg)                    # out: (B x 1 x 1 x 12)\n",
    "        x_templates = self.chord_recognition_templates(x_normalized)        # out: (B x 1 x 1 x 24)\n",
    "        y_pred = self.softmax_temperature(x_templates)                      # out: (B x 1 x 1 x 24)\n",
    "        return y_pred, x_templates, x_normalized, x_avg, x_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f57b5954-1b89-4d47-be67-0f1366f53c54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T15:13:36.043986300Z",
     "start_time": "2025-07-03T15:13:35.970185400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pipeline config\n",
    "\n",
    "compression_params = {'gamma_init': 1.0,       # makes pipeline somehow redundant; input chroma features are already logarithmically compressed\n",
    "                      'trainable': True}\n",
    "\n",
    "temp_smooth_params = {'smoothing_type': 'weighted_sum', \n",
    "                      'avg_length': 51, \n",
    "                      'weight_init': 'uniform'}\n",
    "\n",
    "# temp_smooth_params = {'smoothing_type': 'Gaussian', \n",
    "#                       'avg_length': 51, \n",
    "#                       'sigma_init': 50}\n",
    "\n",
    "feature_norm_params = {'num_features': 12,\n",
    "                       'norm': '2', \n",
    "                       'threshold': 1e-4}\n",
    "\n",
    "chord_template_params = {'shared_weights': True, \n",
    "                         'initialize_parameters': True, \n",
    "                         'normalize_weights': False}\n",
    "\n",
    "softmax_params = {'tau': 1, \n",
    "                  'trainable': False}\n",
    "\n",
    "model_pipeline = chord_recog_templates_pipeline(compression_params=compression_params, \n",
    "                                       temp_smooth_params=temp_smooth_params, \n",
    "                                       feature_norm_params=feature_norm_params, \n",
    "                                       chord_template_params=chord_template_params, \n",
    "                                       softmax_params=softmax_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7730da12-3d4b-433e-aee0-eab3f73ad9af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T15:13:36.573459400Z",
     "start_time": "2025-07-03T15:13:36.472728900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "chord_recog_templates_pipeline           [1, 1, 1, 24]             --\n",
       "├─log_compression: 1-1                   [1, 1, 51, 12]            1\n",
       "├─temporal_smoothing: 1-2                [1, 1, 1, 12]             --\n",
       "│    └─Conv2d: 2-1                       [1, 1, 1, 12]             51\n",
       "├─feature_normalization: 1-3             [1, 1, 1, 12]             --\n",
       "├─chord_recognition_templates: 1-4       [1, 1, 1, 24]             --\n",
       "│    └─Conv2d: 2-2                       [1, 2, 1, 12]             24\n",
       "├─softmax_temperature: 1-5               [1, 1, 1, 24]             (1)\n",
       "==========================================================================================\n",
       "Total params: 77\n",
       "Trainable params: 76\n",
       "Non-trainable params: 1\n",
       "Total mult-adds (M): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.01\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.01\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model_pipeline, input_size=(1, 1, temp_smooth_params['avg_length'], 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603878f2-1b66-4876-9e21-5e0eed15d010",
   "metadata": {},
   "source": [
    "## The END of network configuration\n",
    "## -------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b621ef69-18cc-400b-8228-a66c5f4b85c3",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bae9734c-e276-4757-9f3e-7544aa4ea40f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T15:13:41.875888Z",
     "start_time": "2025-07-03T15:13:41.833003200Z"
    }
   },
   "outputs": [],
   "source": [
    "nonchord = False\n",
    "song_dict = {}\n",
    "# song_dict[0] = ['LetItB', 'r',\n",
    "#                 os.path.join(data_basedir, 'C5', 'FMP_C5_Audio_Beatles_LetItBe_Beatles_1970-LetItBe-06.wav'),\n",
    "#                 os.path.join(data_basedir, 'C5', 'FMP_C5_Audio_Beatles_LetItBe_Beatles_1970-LetItBe-06_Chords_simplified.csv')]\n",
    "song_dict[0] = ['LetItB', 'r',\n",
    "                os.path.join(data_basedir, 'FMP_C5_F01_Beatles_LetItBe-mm1-4_Original.wav'),\n",
    "                os.path.join(data_basedir, 'FMP_C5_F01_Beatles_LetItBe-mm1-4_Original_Chords_simplified.csv')]\n",
    "song_dict[1] = ['HereCo', 'b',\n",
    "                os.path.join(data_basedir, 'FMP_C5_Audio_Beatles_HereComesTheSun_Beatles_1969-AbbeyRoad-07.wav'),\n",
    "                os.path.join(data_basedir, 'FMP_C5_Audio_Beatles_HereComesTheSun_Beatles_1969-AbbeyRoad-07_Chords_simplified.csv')]\n",
    "song_dict[2] = ['ObLaDi', 'c',\n",
    "                os.path.join(data_basedir, 'FMP_C5_Audio_Beatles_ObLaDiObLaDa_Beatles_1968-TheBeatlesTheWhiteAlbumDisc1-04.wav'),\n",
    "                os.path.join(data_basedir, 'FMP_C5_Audio_Beatles_ObLaDiObLaDa_Beatles_1968-TheBeatlesTheWhiteAlbumDisc1-04_Chords_simplified.csv')]\n",
    "song_dict[3] = ['PennyL', 'g',\n",
    "                os.path.join(data_basedir, 'FMP_C5_Audio_Beatles_PennyLane_Beatles_1967-MagicalMysteryTour-09.wav'),\n",
    "                os.path.join(data_basedir, 'FMP_C5_Audio_Beatles_PennyLane_Beatles_1967-MagicalMysteryTour-09_Chords_simplified.csv')]\n",
    "\n",
    "# csv_rel_path = song_dict[0][3]\n",
    "# csv_abs_path = os.path.abspath(csv_rel_path)\n",
    "# print(csv_abs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab115dc0-7282-4db3-a249-28e9dd0104dd",
   "metadata": {},
   "source": [
    "### Feature Extraction (log-compressed STFT-based chroma features as above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc893d2e-5108-46d9-a5b8-fc2e53aff900",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T15:13:53.236309100Z",
     "start_time": "2025-07-03T15:13:44.118533200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Computation of STFT-based chromagrams =====\n",
      "Processing:  LetItB\n",
      "Processing:  HereCo\n",
      "Processing:  ObLaDi\n",
      "Processing:  PennyL\n"
     ]
    }
   ],
   "source": [
    "def compute_X_dict(song_selected, version='STFT', details=True, nonchord=False):\n",
    "    X_dict = {}\n",
    "    Fs_X_dict = {}\n",
    "    ann_dict = {}\n",
    "    x_dur_dict = {}\n",
    "    chord_labels = libfmp.c5.get_chord_labels(ext_minor='m', nonchord=nonchord)\n",
    "    for s in song_selected:\n",
    "        if details is True:\n",
    "            print('Processing: ', song_dict[s][0])\n",
    "        fn_wav = song_dict[s][2]\n",
    "        fn_ann = song_dict[s][3]\n",
    "#         N = 2048\n",
    "#         H = 1024\n",
    "        N = 4096\n",
    "        H = 2048\n",
    "        if version == 'STFT':\n",
    "            X, Fs_X, x, Fs, x_dur = \\\n",
    "                libfmp.c5.compute_chromagram_from_filename(fn_wav, N=N, H=H, gamma=0.1, version='STFT')            # no log compression\n",
    "        if version == 'CQT':\n",
    "            X, Fs_X, x, Fs, x_dur = \\\n",
    "                libfmp.c5.compute_chromagram_from_filename(fn_wav, H=H, version='CQT')                  # no log compression\n",
    "        if version == 'IIR':\n",
    "            X, Fs_X, x, Fs, x_dur = \\\n",
    "                libfmp.c5.compute_chromagram_from_filename(fn_wav, N=N, H=H, gamma=10, version='IIR')             # no log compression\n",
    "            \n",
    "        X_dict[s] = X\n",
    "        Fs_X_dict[s] = Fs_X\n",
    "        x_dur_dict[s] = x_dur\n",
    "        N_X = X.shape[1]\n",
    "        \n",
    "        # one-hot encoding, converts chord label (.csv file) to label matrix\n",
    "        ann_dict[s] = libfmp.c5.convert_chord_ann_matrix(fn_ann, chord_labels, Fs=Fs_X, N=N_X, last=False)   \n",
    "        \n",
    "    return X_dict, Fs_X_dict, ann_dict, x_dur_dict, chord_labels\n",
    "    \n",
    "song_selected = [0, 1, 2, 3]\n",
    "#song_selected = [0]\n",
    "print('===== Computation of STFT-based chromagrams =====')\n",
    "X_dict_STFT, Fs_X_dict_STFT, ann_dict_STFT, x_dur_dict, chord_labels = compute_X_dict(song_selected, version='STFT', nonchord=nonchord)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31357ce-3d70-492e-8b42-c0f14b00d415",
   "metadata": {},
   "source": [
    "### Create Dataset + Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30350ded-fc39-409a-bbaa-dbb0282d259c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T15:13:56.581224500Z",
     "start_time": "2025-07-03T15:13:56.566266200Z"
    }
   },
   "outputs": [],
   "source": [
    "class dataset_context(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs, targets, params):\n",
    "        self.inputs = inputs                                # Channels x Time x Chromas\n",
    "        self.targets = targets                              # Time x Chords\n",
    "        self.context = params['context']\n",
    "        self.stride = params['stride']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.inputs.size()[1] - self.context) // self.stride\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        index *= self.stride\n",
    "        half_context = self.context // 2\n",
    "        index += half_context\n",
    "        X = self.inputs[:, (index - half_context):(index + half_context + 1), :].type(torch.FloatTensor)\n",
    "        # y is the training target(this is strongly aligned label)\n",
    "        y = torch.unsqueeze(torch.unsqueeze(self.targets[index, :], 0), 1).type(torch.FloatTensor)\n",
    "        return X, y\n",
    "    \n",
    "def create_dataset(data_dict, ann_dict, song_dict, song_indices, dataset_params, dataset_description='train'):\n",
    "    all_datasets = []\n",
    "    half_context = dataset_params['context']//2\n",
    "    \n",
    "    for s in song_indices:\n",
    "        inputs = torch.unsqueeze(torch.from_numpy(np.pad(data_dict[s].T, ((half_context, half_context + 1), (0, 0)))), 0)\n",
    "        print(ann_dict[s][0])\n",
    "        # transpose and zero-padding the label matrix\n",
    "        targets = torch.from_numpy(np.pad(ann_dict[s][0].T, ((half_context, half_context + 1), (0, 0))))\n",
    "        curr_dataset = dataset_context(inputs, targets, dataset_params)\n",
    "        all_datasets.append(curr_dataset)\n",
    "        \n",
    "        print(f'- {song_dict[s][0]} added to {dataset_description} set. Length: {len(curr_dataset)} segments')\n",
    "        \n",
    "    full_dataset = torch.utils.data.ConcatDataset(all_datasets)     \n",
    "    print(f'Total number of segments in the {dataset_description} set: {len(full_dataset)}')    \n",
    "    return full_dataset\n",
    "\n",
    "def create_CTC_dataset(data_dict, ann_dict, song_dict, song_indices, dataset_params, dataset_description='train'):\n",
    "    all_datasets = []\n",
    "    half_context = dataset_params['context']//2\n",
    "    \n",
    "    for s in song_indices:\n",
    "        inputs = torch.unsqueeze(torch.from_numpy(np.pad(data_dict[s].T, ((half_context, half_context + 1), (0, 0)))), 0)\n",
    "        print(ann_dict[s][0])\n",
    "        # transpose and zero-padding the label matrix\n",
    "        targets = torch.from_numpy(np.pad(ann_dict[s][0].T, ((half_context, half_context + 1), (0, 0))))\n",
    "        label_sequence = torch.argmax(targets, dim=1)\n",
    "        ctc_targets = [label_sequence[0]]\n",
    "        for i in range(1, len(label_sequence)):\n",
    "            if label_sequence != label_sequence[i-1]:\n",
    "                ctc_targets.append(label_sequence[i])\n",
    "        ctc_targets = torch.tensor(ctc_targets)\n",
    "        target_lengths = torch.tensor([len(ctc_targets)])\n",
    "        curr_dataset = dataset_context(inputs, ctc_targets, dataset_params)\n",
    "        all_datasets.append(curr_dataset)\n",
    "        \n",
    "        print(f'- {song_dict[s][0]} added to {dataset_description} set. Length: {len(curr_dataset)} segments')\n",
    "        \n",
    "    full_dataset = torch.utils.data.ConcatDataset(all_datasets)     \n",
    "    print(f'Total number of segments in the {dataset_description} set: {len(full_dataset)}')    \n",
    "    return full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2ac7dd4-fcc4-49c3-bc37-7ee4336ed548",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T15:13:57.214533100Z",
     "start_time": "2025-07-03T15:13:57.142723600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "- HereCo added to train set. Length: 1999 segments\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "- ObLaDi added to train set. Length: 2034 segments\n",
      "Total number of segments in the train set: 4033\n",
      "\n",
      "\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "- PennyL added to val set. Length: 1975 segments\n",
      "Total number of segments in the val set: 1975\n",
      "\n",
      "\n",
      "-Training data loader contains 81 mini batches.\n",
      "-Validation data loader contains 40 mini batches.\n"
     ]
    }
   ],
   "source": [
    "train_set_indices = [1, 2]\n",
    "train_set_params = {'context': temp_smooth_params['avg_length'], 'stride': 1}\n",
    "\n",
    "train_set = create_dataset(data_dict=X_dict_STFT, \n",
    "                           ann_dict=ann_dict_STFT, \n",
    "                           song_dict=song_dict, \n",
    "                           song_indices=train_set_indices, \n",
    "                           dataset_params=train_set_params, \n",
    "                           dataset_description='train')\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "val_set_indices = [3]\n",
    "val_set_params = {'context': temp_smooth_params['avg_length'], 'stride': 1}\n",
    "\n",
    "val_set = create_dataset(data_dict=X_dict_STFT, \n",
    "                         ann_dict=ann_dict_STFT, \n",
    "                         song_dict=song_dict, \n",
    "                         song_indices=val_set_indices, \n",
    "                         dataset_params=val_set_params, \n",
    "                         dataset_description='val')\n",
    "print('\\n')\n",
    "train_loader_params = {'batch_size': 50, 'shuffle': True, 'num_workers': 0}\n",
    "train_loader = torch.utils.data.DataLoader(train_set, **train_loader_params)\n",
    "print(f'-Training data loader contains {len(train_loader)} mini batches.')\n",
    "\n",
    "val_loader_params = {'batch_size': 50, 'shuffle': False, 'num_workers': 0}\n",
    "val_loader = torch.utils.data.DataLoader(val_set, **val_loader_params)\n",
    "print(f'-Validation data loader contains {len(val_loader)} mini batches.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a387db1-3d2e-47f7-99a7-6f5ae4f1207d",
   "metadata": {},
   "source": [
    "## Train Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "751b204c-9ceb-4774-9eca-669d22e37676",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T15:14:00.508947500Z",
     "start_time": "2025-07-03T15:14:00.465064100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters:\n",
      "\n",
      "'log_compression.log_gamma'\n",
      "'temporal_smoothing.filter.weight'\n",
      "'chord_recognition_templates.filter.weight'\n"
     ]
    }
   ],
   "source": [
    "print('Trainable parameters:\\n')\n",
    "\n",
    "trainable_parameters = []\n",
    "for name, param in model_pipeline.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        trainable_parameters.append(name)\n",
    "        print(f\"'{name}'\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14bbf2cc-0f76-4f6b-b428-ea74f40fe55d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T15:14:01.142248800Z",
     "start_time": "2025-07-03T15:14:01.115321Z"
    }
   },
   "outputs": [],
   "source": [
    "optimization_mode = 0\n",
    "\n",
    "if optimization_mode == 1:\n",
    "    frozen_parameters = []           \n",
    "\n",
    "if optimization_mode == 2:\n",
    "    trained_parameters = []\n",
    "if optimization_mode == 0:\n",
    "    parameters_to_optimize = model_pipeline.parameters()\n",
    "    \n",
    "elif optimization_mode == 1:\n",
    "    parameters_to_optimize = []\n",
    "\n",
    "    for name, param in model_pipeline.named_parameters():\n",
    "        if not name in frozen_parameters:\n",
    "            parameters_to_optimize.append(param)\n",
    "            \n",
    "elif optimization_mode == 2:\n",
    "    parameters_to_optimize = []\n",
    "\n",
    "    for name, param in model_pipeline.named_parameters():\n",
    "        if name in trained_parameters:\n",
    "            parameters_to_optimize.append(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44144859-fc7b-44a3-9795-d6e6fc2f929f",
   "metadata": {},
   "source": [
    "### Optimizer&Training params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97b8d740-f325-4ade-8694-77d6b72bf850",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T15:14:05.619356700Z",
     "start_time": "2025-07-03T15:14:02.309978600Z"
    }
   },
   "outputs": [],
   "source": [
    "optim_params = {'lr': 0.01, 'betas': (0.9, 0.999)}\n",
    "optimizer = torch.optim.Adam(parameters_to_optimize, **optim_params)\n",
    "training_params = {'device': 'cpu', 'max_epochs': 50}\n",
    "# training_params = {'device': 'cuda:0', 'max_epochs': 25}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a1ee4c-fe74-47f7-8b8c-922140d7b216",
   "metadata": {},
   "source": [
    "# CTC LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a61da83f-ed64-4da3-9bbe-1107fb5762b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ctc_loss_fn(log_probs, targets, input_lengths, target_lengths):\n",
    "    return F.ctc_loss(\n",
    "        log_probs=log_probs,\n",
    "        targets=targets,\n",
    "        input_lengths=input_lengths,\n",
    "        target_lengths=target_lengths,\n",
    "        blank=0,                 # 0 as blank index\n",
    "        reduction='mean',\n",
    "        zero_infinity=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b81cfb-214e-4a1b-a0f6-e19a9eb4b5fe",
   "metadata": {},
   "source": [
    "### Network Training - Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e604a039-5146-40c8-8c47-15ca96e28995",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-2, 1], but got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6804\\2351853340.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_pipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m                       \u001b[1;31m# [B, 1, T, 24]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m                       \u001b[1;31m# [B, T, 24]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[0mlog_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m            \u001b[1;31m# [B, T, 24]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[0mlog_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m           \u001b[1;31m# [T, B, 24]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
     ]
    }
   ],
   "source": [
    "model_pipeline = model_pipeline.to(training_params['device'])\n",
    "\n",
    "best_model = deepcopy(model_pipeline)\n",
    "best_val_loss = None\n",
    "best_epoch = None\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(training_params['max_epochs']):\n",
    "    model_pipeline.train()\n",
    "\n",
    "    with torch.enable_grad():\n",
    "        accum_loss, n_batches = 0, 0\n",
    "        for X, y in train_loader:\n",
    "            X, y = X.to(training_params['device']), y.to(training_params['device'])\n",
    "\n",
    "            y_pred = model_pipeline(X)                       # [B, 1, T, 24]\n",
    "            logits = logits.squeeze(1)                       # [B, T, 24]\n",
    "            log_probs = logits.log_softmax(dim=2)            # [B, T, 24]\n",
    "            log_probs = log_probs.permute(1, 0, 2)           # [T, B, 24]\n",
    "\n",
    "            # Convert one-hot targets to class indices\n",
    "            target_classes = torch.argmax(y.squeeze(1).squeeze(1), dim=1)  # [B]\n",
    "            targets = target_classes\n",
    "\n",
    "            input_lengths = torch.full(\n",
    "                size=(log_probs.size(1),), fill_value=log_probs.size(0), dtype=torch.long\n",
    "            )\n",
    "            target_lengths = torch.ones(log_probs.size(1), dtype=torch.long)\n",
    "\n",
    "            loss = ctc_loss_fn(log_probs, targets, input_lengths, target_lengths)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            accum_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "    model_pipeline.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        accum_loss_val, n_batches_val = 0, 0\n",
    "        for X_val, y_val in val_loader:\n",
    "            X_val, y_val = X_val.to(training_params['device']), y_val.to(training_params['device'])\n",
    "\n",
    "            logits_val = model_pipeline(X_val)                       # [B, 1, T, 24]\n",
    "            logits_val = logits_val.squeeze(1)                       # [B, T, 24]\n",
    "            log_probs_val = logits_val.log_softmax(dim=2)            # [B, T, 24]\n",
    "            log_probs_val = log_probs_val.permute(1, 0, 2)           # [T, B, 24]\n",
    "\n",
    "            target_classes_val = torch.argmax(y_val.squeeze(1).squeeze(1), dim=1)  # [B]\n",
    "            targets_val = target_classes_val\n",
    "\n",
    "            input_lengths_val = torch.full(\n",
    "                size=(log_probs_val.size(1),), fill_value=log_probs_val.size(0), dtype=torch.long\n",
    "            )\n",
    "            target_lengths_val = torch.ones(log_probs_val.size(1), dtype=torch.long)\n",
    "\n",
    "            loss_val = ctc_loss_fn(log_probs_val, targets_val, input_lengths_val, target_lengths_val)\n",
    "\n",
    "            accum_loss_val += loss_val.item()\n",
    "            n_batches_val += 1\n",
    "\n",
    "    train_loss = accum_loss / n_batches\n",
    "    val_loss = accum_loss_val / n_batches_val\n",
    "    print(f'Finished epoch {epoch}. Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "    if best_val_loss is None or val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = deepcopy(model_pipeline)\n",
    "        best_epoch = epoch\n",
    "        print('    ...saved model')\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "model_pipeline = deepcopy(best_model)\n",
    "print(f'\\nRestored model from epoch {best_epoch}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "68c33ed7-153f-41f9-9d80-5deb39ebd710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if len(train_losses) > 1:\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Curve')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d53586fe934af5b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1c1413f5544756",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T15:15:39.384453100Z",
     "start_time": "2025-07-03T15:15:39.253788500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
